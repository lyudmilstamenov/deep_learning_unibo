{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElNaMbLnRdHR"
      },
      "source": [
        "# Sentence Reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXr4iGUGRms8"
      },
      "source": [
        "The purpose of this project is to take in input a sequence of words corresponding to a random permutation of a given english sentence, and reconstruct the original sentence.\n",
        "\n",
        "The otuput can be either produced in a single shot, or through an iterative (autoregressive) loop generating a single token at a time.\n",
        "\n",
        "\n",
        "CONSTRAINTS:\n",
        "* No pretrained model can be used.\n",
        "* The neural network models should have less the 20M parameters.\n",
        "* No postprocessing should be done (e.g. no beamsearch)\n",
        "* You cannot use additional training data.\n",
        "\n",
        "\n",
        "BONUS PARAMETERS:\n",
        "\n",
        "A bonus of 0-2 points will be attributed to incentivate the adoption of models with a low number of parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ8k-L-WUK7l"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "The dataset is composed by sentences taken from the generics_kb dataset of hugging face. We restricted the vocabolary to the 10K most frequent words, and only took sentences making use of this vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nJ02vehGYySk",
        "outputId": "57a606d3-b8c8-4948-dcd9-5473b34dea84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/542.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/542.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the dataset"
      ],
      "metadata": {
        "id": "807Wk-ir_bDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from keras.layers import TextVectorization\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "ds = load_dataset('generics_kb',trust_remote_code=True)['train']"
      ],
      "metadata": {
        "id": "_WjtqA8TrHcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter row with length greater than 8.\n"
      ],
      "metadata": {
        "id": "lAVLfsdc_ej5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.filter(lambda row: len(row[\"generic_sentence\"].split(\" \"))>8 )\n",
        "corpus = [ '<start> ' + row['generic_sentence'].replace(\",\",\" <comma>\") + ' <end>' for row in ds ]\n",
        "corpus = np.array(corpus)\n"
      ],
      "metadata": {
        "id": "iznq8xGNt2Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a tokenizer and Detokenizer"
      ],
      "metadata": {
        "id": "FyYpXLCF_ldR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=TextVectorization( max_tokens=10000, standardize=\"lower_and_strip_punctuation\", encoding=\"utf-8\",) #con il max prende le piu frequenti. ordina i token del vocab dal piu frequente al meno frequente\n",
        "tokenizer.adapt(corpus)\n",
        "\n",
        "class TextDetokenizer:\n",
        "    def __init__(self, vectorize_layer):\n",
        "        self.vectorize_layer = vectorize_layer\n",
        "        vocab = self.vectorize_layer.get_vocabulary()\n",
        "        self.index_to_word = {index: word for index, word in enumerate(vocab)}\n",
        "\n",
        "    def __detokenize_tokens(self, tokens):\n",
        "        def check_token(t):\n",
        "          if t == 3:\n",
        "            s=\"<start>\"\n",
        "          elif t == 2:\n",
        "            s=\"<end>\"\n",
        "          elif t == 7:\n",
        "            s=\"<comma>\"\n",
        "          else:\n",
        "            s=self.index_to_word.get(t, '[UNK]')\n",
        "          return s\n",
        "\n",
        "        return ' '.join([ check_token(token) for token in tokens if token != 0])\n",
        "\n",
        "    def __call__(self, batch_tokens):\n",
        "       return [self.__detokenize_tokens(tokens) for tokens in batch_tokens]\n",
        "\n",
        "\n",
        "\n",
        "detokenizer = TextDetokenizer( tokenizer )\n",
        "sentences = tokenizer( corpus ).numpy()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "T-bE2JpVbU9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove from corpus the sentences where any unknow word appears"
      ],
      "metadata": {
        "id": "lZ64sns1_pSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.sum( (sentences==1) , axis=1) >= 1\n",
        "original_data = np.delete( sentences, mask , axis=0)"
      ],
      "metadata": {
        "id": "2LPQtryQz5wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_data.shape"
      ],
      "metadata": {
        "id": "qYfOscVk7U0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffle the sentences"
      ],
      "metadata": {
        "id": "5puiiQ2D_uxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_full_data(generator):\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    for i in range(len(generator)):\n",
        "        x_batch, y_batch = generator[i]\n",
        "        x_list.append(x_batch)\n",
        "        y_list.append(y_batch)\n",
        "    x = np.concatenate(x_list, axis=0)\n",
        "    y = np.concatenate(y_list, axis=0)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "lSxaLztEQBg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, data, batch_size=32, shuffle=True):\n",
        "\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.data) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        data_batch = np.array([self.data[k] for k in indexes])\n",
        "        #copy of ordered sequences\n",
        "        result = np.copy(data_batch)\n",
        "        #shuffle only the relevant positions for each batch\n",
        "        for i in range(data_batch.shape[0]):\n",
        "          np.random.shuffle(data_batch[i,1:data_batch[i].argmin() - 1])\n",
        "\n",
        "        return data_batch , result\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(len(self.data))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)"
      ],
      "metadata": {
        "id": "1ZXLkWB6od0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = DataGenerator(original_data[:220000])\n",
        "test_generator = DataGenerator(original_data[220000:])\n",
        "x, y = test_generator.__getitem__(1)"
      ],
      "metadata": {
        "id": "uNlq1Khx1oH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer.get_vocabulary()"
      ],
      "metadata": {
        "id": "bsyic8wYJPJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = detokenizer(x)\n",
        "y = detokenizer(y)\n",
        "\n",
        "for i in range(7):\n",
        "  print(\"original: \", y[i])\n",
        "  print(\"shuffled: \", x[i])\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "5yiFpF_9HFsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def score(s,p):\n",
        "  match = SequenceMatcher(None, s, p).find_longest_match()\n",
        "  #print(match.size)\n",
        "  return (match.size/max(len(p),len(s)))"
      ],
      "metadata": {
        "id": "jXRPyCS2HH8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original = \"at first henry wanted to be friends with the king of france\"\n",
        "generated = \"henry wanted to be friends with king of france at the first\"\n",
        "\n",
        "print(\"your score is \",score(original,generated))"
      ],
      "metadata": {
        "id": "A7kh_DTzHJJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = extract_full_data(train_generator)\n",
        "x_train.shape, y_train.shape"
      ],
      "metadata": {
        "id": "AqK_VBMERIr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test, y_test = extract_full_data(test_generator)\n",
        "x_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "Gp0_kxvBRJpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = tokenizer.get_vocabulary()"
      ],
      "metadata": {
        "id": "loy2C8odT2Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = extract_full_data(train_generator)\n",
        "x_train.shape, x_train.shape"
      ],
      "metadata": {
        "id": "HbQTmbHmX_u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical, pad_sequences\n",
        "def  prepare_dataset(X, Y):\n",
        "  c_set = pad_sequences(np.array([s[1:] for s in X]), maxlen=28, padding='post')\n",
        "  x_set = Y\n",
        "  y_set = pad_sequences(np.array([s[1:] for s in Y]), maxlen=28, padding='post')\n",
        "  context = []\n",
        "  labels = []\n",
        "  inputs= []\n",
        "\n",
        "  for j,x in enumerate(x_set):\n",
        "    non_null_count = sum(x>0)\n",
        "    for i in range(non_null_count):\n",
        "      context.append(c_set[j])\n",
        "      inputs.append(pad_sequences([x[:i+1]], maxlen=28, padding='post')[0])\n",
        "      labels.append(y_set[j])\n",
        "\n",
        "  return np.array(context), np.array(inputs), np.array(labels)\n"
      ],
      "metadata": {
        "id": "BDz_RRaCRRTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_train, inputs_train, labels_train = prepare_dataset(x_train[:9000], y_train[:9000])\n",
        "context_val, inputs_val, labels_val = prepare_dataset(x_train[9000:10000], y_train[9000:10000])\n",
        "context_test, inputs_test, labels_test = prepare_dataset(x_test[:10000], y_test[:10000])"
      ],
      "metadata": {
        "id": "DbICPbKkSn48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras.backend as k\n",
        "def positional_encoding(length, depth):\n",
        "    depth = depth/2\n",
        "\n",
        "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "    angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "    pos_encoding = np.concatenate(\n",
        "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "        axis=-1)\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.add = tf.keras.layers.Add()\n",
        "class CrossAttention(BaseAttention):\n",
        "    def call(self, x, context):\n",
        "        attn_output, att_scores = self.mha(\n",
        "            query=x,\n",
        "            key=context,\n",
        "            value=context,\n",
        "            return_attention_scores=True\n",
        "        )\n",
        "        self.last_attn_scores = att_scores\n",
        "\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "        return x\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedder, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = embedder\n",
        "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "    def compute_mask(self, *args, **kwargs):\n",
        "        return self.embedding.compute_mask(*args, **kwargs)\n",
        "    def call(self, x):\n",
        "        length = tf.shape(x)[1]\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[tf.newaxis, :length, :]\n",
        "        return x\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "    def call(self, x):\n",
        "        attn_output = self.mha(\n",
        "            query=x,\n",
        "            value=x,\n",
        "            key=x\n",
        "        )\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "        return x\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, embedder, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = embedder\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # The last attention scores are cached for later plotting\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, embedder, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(embedder, vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.embedder = tf.keras.layers.Embedding(input_vocab_size, d_model, mask_zero=True)\n",
        "    self.encoder = Encoder(self.embedder,num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(self.embedder,num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # All inputs must be passed in the first argument to use '.fit'\n",
        "\n",
        "    context, x  = inputs\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Keras mask is dropped, so it doesn't scale with losses or metrics.\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "\n",
        "# num_layers = 8\n",
        "num_layers = 4\n",
        "# d_model = 128\n",
        "d_model = 128\n",
        "# dff = 512\n",
        "dff = 128\n",
        "# num_heads = 8\n",
        "num_heads = 8\n",
        "dropout_rate = 0.2\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=10_000,\n",
        "    target_vocab_size=10_000,\n",
        "    dropout_rate=dropout_rate)\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "K_VALUE = 0.97\n",
        "\n",
        "# Defining a custom loss function that works directly on tokens\n",
        "def custom_masked_loss(label, pred):\n",
        "\n",
        "    mask = label != 0\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_object(label, pred)\n",
        "\n",
        "    final_array = tf.pow(K_VALUE,tf.cast(tf.range(1,28+1),tf.float32))\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    mask*=final_array\n",
        "\n",
        "    loss *= mask\n",
        "\n",
        "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "    return loss\n",
        "\n",
        "# Defining a custom metric that works directly on tokens\n",
        "def masked_accuracy(label, pred):\n",
        "    pred = tf.argmax(pred, axis=2)\n",
        "    label = tf.cast(label, pred.dtype)\n",
        "    match = label == pred\n",
        "\n",
        "    mask = label != 0\n",
        "\n",
        "    match = match & mask\n",
        "\n",
        "    match = tf.cast(match, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n",
        "\n",
        "transformer.compile(\n",
        "    loss=custom_masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])\n",
        "\n",
        "transformer.build(input_shape = [(None, 28), (None, 28)])\n",
        "\n",
        "transformer.summary()"
      ],
      "metadata": {
        "id": "JDUXVP-ScbTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = transformer.predict((np.array([context_train[0]]), np.array([inputs_train[0]])))\n",
        "tf.argmax(result, axis=2), context_train[0]"
      ],
      "metadata": {
        "id": "tpxwXkYcdBEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(custom_masked_loss(y_train[0],result))"
      ],
      "metadata": {
        "id": "aDUhVKZUeajO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "2oUY4r55cEIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# # Check for available GPUs\n",
        "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "# if gpus:\n",
        "#     try:\n",
        "#         # Enable memory growth for each GPU\n",
        "#         for gpu in gpus:\n",
        "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
        "#         logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "#         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "#     except RuntimeError as e:\n",
        "#         # Memory growth must be set before GPUs have been initialized\n",
        "#         print(e)"
      ],
      "metadata": {
        "id": "FZoLzOXYcSwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Callbacks\n",
        "es = EarlyStopping(monitor='val_masked_accuracy', mode='max', verbose=1, patience=10)\n",
        "\n",
        "epochs = 50\n",
        "batch_size= 512\n",
        "\n",
        "# Training phase\n",
        "history = transformer.fit(\n",
        "    (context_train, inputs_train),\n",
        "    labels_train,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    # callbacks = [es],\n",
        "    validation_data = ((context_val, inputs_val), labels_val)\n",
        ")"
      ],
      "metadata": {
        "id": "rwj_eBJyd6AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TkWdvm6BYIh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def score(s, p):\n",
        "    match = SequenceMatcher(None, s, p).find_longest_match(0, len(s), 0, len(p))\n",
        "    return match.size / max(len(p), len(s))\n",
        "\n",
        "# Limit the number of examples for testing\n",
        "\n",
        "\n",
        "def get_score(x, y):\n",
        "\n",
        "  max_count = sum(x > 0) - 1\n",
        "  generated = pad_sequences(np.array([x[:1]]), maxlen=28, padding='post')[0]\n",
        "  available_tokens = x[1:sum((x>0))].tolist()\n",
        "  original = pad_sequences([y[1:]], maxlen=28, padding='post')[0]\n",
        "  for count in range(max_count):\n",
        "    current_input = x\n",
        "    prediction = transformer.predict((np.array([generated]), np.array([current_input])), verbose=0)\n",
        "\n",
        "    mask = x > 0\n",
        "\n",
        "    relevant_logits = prediction[0, count, available_tokens]\n",
        "\n",
        "    generated_index = np.argmax(relevant_logits, axis=-1)\n",
        "    generated_token = available_tokens[generated_index]\n",
        "\n",
        "    available_tokens.remove(generated_token)\n",
        "    generated[count+1] = generated_token\n",
        "\n",
        "  generated = generated[:sum(generated>0)]\n",
        "  original = original[:sum(original>0)]\n",
        "  # print(generated)\n",
        "  return score(original, generated)\n",
        "\n",
        "num_examples = 10\n",
        "\n",
        "for i in range(num_examples):\n",
        "  a = x_train[i][:sum(x_train[i]>0)]\n",
        "  b = y_train[i][:sum(y_train[i]>0)]\n",
        "  print(f'Random: {score(b, a)}')\n",
        "  # print(b)\n",
        "  # print(a)\n",
        "  print(f'Generated: {get_score(x_train[i],y_train[i])}')\n",
        "    # print(f\"Original:  {original}\")\n",
        "    # print(f\"Generated: {generated}\")\n",
        "    # print(f\"Score: {score(original, generated.tolist())}\")"
      ],
      "metadata": {
        "id": "oRhriVl8V_pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RcntvuUQgbm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_x[1:sum((current_x>0))]\n"
      ],
      "metadata": {
        "id": "_0sLyBDGaedM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(prediction[:,:,current_x[1:sum((current_x>0))]], axis=2) * mask"
      ],
      "metadata": {
        "id": "chtKp1dDaGLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = transformer.predict((np.array([context_train[1]]), np.array([input_train[1]])))\n",
        "tf.argmax(result, axis=2), input_train[1], labels_train[1]"
      ],
      "metadata": {
        "id": "MTBc-9KSmQi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_tokens"
      ],
      "metadata": {
        "id": "hhkpX8oGEK3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = len(vocabulary)\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "nuvJ5RkhYciw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator = ModifiedDataGenerator((x_test,y_test), batch_size=batch_size, vocabulary_size=vocabulary_size)\n",
        "train_generator = ModifiedDataGenerator((x_train,y_train), batch_size=batch_size, vocabulary_size=vocabulary_size)\n"
      ],
      "metadata": {
        "id": "VWKTreHjZr6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pLTKIOoSbu9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo8MazCGBTv3"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0NOkuO0CfPo"
      },
      "source": [
        "Let s be the source string and p your prediction. The quality of the results will be measured according to the following metric:\n",
        "\n",
        "1.  look for the longest substring w between s and p\n",
        "2.  compute |w|/max(|s|,|p|)\n",
        "\n",
        "If the match is exact, the score is 1.\n",
        "\n",
        "When computing the score, you should NOT consider the start and end tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-aUrdlXDdVf"
      },
      "source": [
        "The longest common substring can be computed with the SequenceMatcher function of difflib, that allows a simple definition of our metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB2YfjXNExM-"
      },
      "source": [
        "Let's do an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h17C8bVjEwur"
      },
      "outputs": [],
      "source": [
        "original = \"at first henry wanted to be friends with the king of france\"\n",
        "generated = \"henry wanted to be friends with king of france at the first\"\n",
        "\n",
        "print(\"your score is \",score(original,generated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BET8GqBvFugR"
      },
      "source": [
        "The score must be computed as an average of at least 3K random examples taken form the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fwo7xj4GBW1"
      },
      "source": [
        "# What to deliver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6uITuxOGHfJ"
      },
      "source": [
        "You are supposed to deliver a single notebook, suitably commented.\n",
        "The notebook should describe a single model, although you may briefly discuss additional attempts you did.\n",
        "\n",
        "The notebook should contain a full trace of the training.\n",
        "Weights should be made available on request.\n",
        "\n",
        "You must also give a clear assesment of the performance of the model, computed with the metric that has been given to you.\n",
        "\n",
        "# Good work!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}